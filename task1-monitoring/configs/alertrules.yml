groups:
  - name: platform.canary
    rules:
      - alert: DeadMansSwitch
        expr: vector(1)
        labels: { severity: ticket, team: platform }
        annotations:
          summary: "DeadMansSwitch"
          description: "Always firing; if it stops, alert pipeline is broken."

  - name: platform.health
    rules:
      - alert: InstanceDown
        expr: up == 0
        for: 2m
        labels: { severity: page, team: platform }
        annotations:
          summary: "Instance down: {{ $labels.instance }}"
          description: "Target {{ $labels.job }}/{{ $labels.instance }} is down."

      - alert: HighCPU
        expr: 100 * (1 - avg by (instance)(rate(node_cpu_seconds_total{mode="idle"}[5m]))) > 90
        for: 10m
        labels: { severity: warn, team: platform }
        annotations:
          summary: "High CPU > 90%: {{ $labels.instance }}"
          description: "Sustained CPU > 90% for 10m."

      - alert: HighMemory
        expr: 100 * (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) > 90
        for: 10m
        labels: { severity: warn, team: platform }
        annotations:
          summary: "High memory > 90%: {{ $labels.instance }}"
          description: "Sustained memory pressure > 90% for 10m."

      - alert: DiskFillingSoon
        expr: predict_linear(node_filesystem_free_bytes{fstype!~"tmpfs|overlay|squashfs"}[6h], 4*3600) < 0
        for: 10m
        labels: { severity: page, team: platform }
        annotations:
          summary: "Disk likely full in <4h: {{ $labels.instance }}"
          description: "Free space predicted to hit 0 within 4 hours."

  - name: app.latency_errors
    rules:
      - alert: HTTPErrorRateHigh
        expr: |
          sum by (job) (rate(http_requests_total{code=~"5.."}[5m])) /
          sum by (job) (rate(http_requests_total[5m])) > 0.05
        for: 10m
        labels: { severity: page, team: app }
        annotations:
          summary: "5xx > 5% for 10m (job={{ $labels.job }})"
          description: "Elevated server error rate sustained for 10 minutes."

      - alert: HTTPLatencyP95Above
        expr: |
          histogram_quantile(0.95, sum by (le) (rate(http_request_duration_seconds_bucket[5m]))) > 0.5
        for: 10m
        labels: { severity: warn, team: app }
        annotations:
          summary: "P95 latency > 500ms (job={{ $labels.job }})"
          description: "User-facing latency elevated for 10 minutes."

  - name: slo.budget
    rules:
      - alert: SLOFastBurn
        expr: |
          (sum(rate(http_requests_total{code=~"5.."}[5m])) /
           sum(rate(http_requests_total[5m]))) > (0.001 * 14.4)
        for: 5m
        labels: { severity: page, team: app, slo: "availability-99.9" }
        annotations:
          summary: "Fast SLO burn (job={{ $labels.job }})"
          description: "Consuming >14.4× error budget over 5 minutes."

      - alert: SLOSlowBurn
        expr: |
          (sum(rate(http_requests_total{code=~"5.."}[1h])) /
           sum(rate(http_requests_total[1h]))) > (0.001 * 6)
        for: 1h
        labels: { severity: ticket, team: app, slo: "availability-99.9" }
        annotations:
          summary: "Slow SLO burn (job={{ $labels.job }})"
          description: "Consuming >6× error budget over 1 hour."
